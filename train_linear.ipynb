{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import List, Tuple\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "from jax import Array\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "from function_learning_task import init_function_learning_task, step_function_learning_task\n",
    "from training import train_on_sequence\n",
    "from utils import tree_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiftTDState(eqx.Module):\n",
    "    # Static params\n",
    "    n_features: int = eqx.field(static=True) # Number of input features\n",
    "    meta_lr: float = eqx.field(static=True) # Meta learning rate\n",
    "\n",
    "    epsilon: float = eqx.field(static=True) # LR decay factor\n",
    "    eta: float = eqx.field(static=True) # Max learning rate\n",
    "    trace_decay: float = eqx.field(static=True) # Lambda trace decay\n",
    "    gamma: float = eqx.field(static=True) # Discount factor\n",
    "\n",
    "    # State vars\n",
    "    beta: Array # Learning rate exponent\n",
    "    h_old: Array\n",
    "    h_temp: Array\n",
    "    z_delta: Array\n",
    "    p: Array # Eligibility trace of lr exponent\n",
    "    h: Array\n",
    "    z: Array # Eligibility trace of weights\n",
    "    z_bar: Array\n",
    "    V_delta: Array\n",
    "    V_old: Array\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_features,\n",
    "            lr_init: float = 1e-7,\n",
    "            meta_lr: float = 1e-3,\n",
    "            epsilon: float = 0.9,\n",
    "            eta: float = 0.5,\n",
    "            trace_decay: float = 0.95,\n",
    "            gamma: float = 0.1,\n",
    "        ):\n",
    "        self.n_features = n_features\n",
    "        self.beta = jnp.log(lr_init) * jnp.ones(self.n_features)\n",
    "        self.meta_lr = meta_lr\n",
    "        self.epsilon = epsilon\n",
    "        self.eta = eta\n",
    "        self.trace_decay = trace_decay\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.h_old, self.h_temp, self.z_delta, self.p, self.h, self.z, self.z_bar = [jnp.zeros(self.n_features) for _ in range(7)]\n",
    "        self.V_delta, self.V_old = [jnp.array(0.0) for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swift_td_step(\n",
    "        state: SwiftTDState,\n",
    "        weights: Array,\n",
    "        features: Array,\n",
    "        cumulant: float,\n",
    "    ) -> Tuple[SwiftTDState, Array, float]:\n",
    "    \"\"\"SwiftTD update step.\n",
    "    \n",
    "    Args:\n",
    "        state (SwiftTDState): Current state of the Swift-TD algorithm.\n",
    "        weights (Array): Current weights of the model. Must have elements of shape (n_features,).\n",
    "        features (Array): Input features.\n",
    "        cumulant (float): Scalar cumulant signal.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[SwiftTDState, Array, float]: Updated state, updated weights, and TD error.\n",
    "    \"\"\"\n",
    "    orig_weight_shape = weights.shape\n",
    "    weights = weights.flatten()\n",
    "    V = jnp.dot(weights, features)\n",
    "    delta = cumulant + state.gamma * V - state.V_old\n",
    "\n",
    "    # Weight and lr updates\n",
    "    out = {}\n",
    "    delta_w = delta * state.z - state.z_delta * state.V_delta\n",
    "    delta_w = jnp.where(state.z == 0, jnp.zeros_like(features), delta_w)\n",
    "    weights = jnp.where(state.z == 0, weights, weights + delta_w) # Weight update\n",
    "\n",
    "    out['beta'] = state.beta + state.meta_lr / (jnp.exp(state.beta) + 1e-8) # Meta learning rate update\n",
    "    out['beta'] = jnp.minimum(out['beta'], jnp.log(state.eta)) # Clip learning rate\n",
    "    out['h_old'] = state.h\n",
    "    out['h'] = state.h_temp\n",
    "    out['h_temp'] = out['h'] + delta * state.z_bar - state.z_delta * state.V_delta\n",
    "    out['z_delta'] = jnp.zeros_like(state.z_delta)\n",
    "\n",
    "    # Decay traces\n",
    "    out['z'] = state.gamma * state.trace_decay * state.z\n",
    "    out['p'] = state.gamma * state.trace_decay * state.p\n",
    "    out['z_bar'] = state.gamma * state.trace_decay * state.z_bar\n",
    "\n",
    "    # Replace state variables with out values only where z != 0\n",
    "    state = tree_replace(\n",
    "        state,\n",
    "        **{k: jnp.where(state.z == 0, getattr(state, k), v) for k, v in out.items()}\n",
    "    )\n",
    "\n",
    "    state = tree_replace(state, V_delta=jnp.array(0.0))\n",
    "    lr = jnp.exp(state.beta)\n",
    "    E = jnp.maximum(jnp.array(state.eta), jnp.dot(lr, features ** 2)) # Rate of learning\n",
    "    T = jnp.dot(state.z, features)\n",
    "    state = tree_replace(state, V_delta=state.V_delta + jnp.dot(delta_w, features)) # Minor error because delta_w may not be defined\n",
    "\n",
    "    # Eligibility trace updates\n",
    "    out = {}\n",
    "    out['z_delta'] = state.eta / E * jnp.exp(state.beta) * features\n",
    "    out['z'] = state.z + out['z_delta'] * (1 - T) # Update weight eligibility trace\n",
    "    out['p'] = state.p + features * state.h # Update lr eligibility trace\n",
    "    out['z_bar'] = state.z_bar + out['z_delta'] * (1 - T - features * state.z_bar)\n",
    "    out['h_temp'] = state.h_temp - state.h_old * features * (out['z'] - out['z_delta']) \\\n",
    "        - state.h * out['z_delta'] * features\n",
    "    \n",
    "    # Conditionally decay lr\n",
    "    out['beta'] = jax.lax.cond(\n",
    "        E <= state.eta,\n",
    "        lambda beta: beta,\n",
    "        lambda beta: beta + jnp.abs(features) * jnp.log(state.epsilon),\n",
    "        state.beta,\n",
    "    )\n",
    "    out['h_temp'], out['h'], out['z_bar'] = jax.lax.cond(\n",
    "        E <= state.eta,\n",
    "        lambda h_temp, h, z_bar: (h_temp, h, z_bar),\n",
    "        lambda h_temp, h, z_bar: (jnp.zeros_like(h_temp), jnp.zeros_like(h), jnp.zeros_like(z_bar)),\n",
    "        out['h_temp'], state.h, out['z_bar'],\n",
    "    )\n",
    "\n",
    "    # Replace state variables with out values only where features != 0\n",
    "    state = tree_replace(\n",
    "        state,\n",
    "        **{k: jnp.where(features == 0, getattr(state, k), v) for k, v in out.items()}\n",
    "    )\n",
    "\n",
    "    state = tree_replace(state, V_old=V)\n",
    "\n",
    "    return state, weights.reshape(orig_weight_shape), delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_state = SwiftTDState(n_features=2, gamma=0)\n",
    "weights = jnp.array([0.0, 0.0])\n",
    "true_weights = jnp.array([-1.0, 1.0])\n",
    "\n",
    "feature_sequence = jnp.tile(jnp.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0], [0.0, 0.0]]), (10, 1))\n",
    "order = jax.random.permutation(jax.random.PRNGKey(0), len(feature_sequence))\n",
    "feature_sequence = feature_sequence[order]\n",
    "cumulant_sequence = jnp.dot(feature_sequence, true_weights)\n",
    "\n",
    "# Randomize order\n",
    "feature_sequence = feature_sequence[1:]\n",
    "cumulant_sequence = cumulant_sequence[:-1]\n",
    "\n",
    "update_fn = jax.jit(swift_td_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002597332\n",
      "0.004532099\n",
      "0.009252369\n",
      "0.008685052\n",
      "0.0\n",
      "0.0\n",
      "-0.007547915\n",
      "0.0\n",
      "0.0\n",
      "0.005201757\n",
      "0.0\n",
      "0.00401783\n",
      "-0.005837679\n",
      "-0.005837679\n",
      "-0.0045090914\n",
      "-0.00037896633\n",
      "0.0\n",
      "0.0030030608\n",
      "0.00022655725\n",
      "0.0023105145\n",
      "-0.0002501607\n",
      "0.0018559098\n",
      "0.0020457506\n",
      "-0.0009137988\n",
      "0.0\n",
      "-0.0021799803\n",
      "0.0015004277\n",
      "-0.0016685724\n",
      "0.0\n",
      "-0.00013160706\n",
      "-0.00013160706\n",
      "-0.0012443662\n",
      "-0.0012279749\n",
      "0.00025689602\n",
      "-0.0007109046\n",
      "0.0010761023\n",
      "0.0\n",
      "0.0\n",
      "-0.00062686205\n",
      "[-0.99952525  0.9991839 ]\n"
     ]
    }
   ],
   "source": [
    "for features, cumulant in zip(feature_sequence, cumulant_sequence):\n",
    "    std_state, weights, td_error = update_fn(std_state, weights, features, cumulant)\n",
    "    print(td_error)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "        1.,  1.,  1., -1., -1.,  0.,  0.,  0.,  0., -1., -1.,  0., -1.,\n",
       "        0.,  1., -1., -1.,  1.,  0.,  1., -1.,  1.,  0.,  0.,  1., -1.,\n",
       "        0.,  0.,  1.,  0.,  0., -1.,  0.,  1.,  0.,  1., -1.,  0.,  1.,\n",
       "        0.,  0.,  0.,  1.,  0., -1.,  1.,  0.,  0.,  1., -1.,  1.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0., -1.,  0.,\n",
       "        1.,  0.,  1., -1.,  0.,  0., -1.,  0., -1.,  0.,  1.,  0.,  1.,\n",
       "        0., -1., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1.,  1.,\n",
       "       -1.,  1.,  0.,  1., -1., -1.,  0., -1.,  1.,  0.,  1.,  1.,  1.,\n",
       "        1., -1., -1.,  0., -1., -1.,  0.,  0., -1.,  1., -1.,  1.,  0.,\n",
       "        0.,  0.,  1., -1.,  0.,  0.,  0.,  1., -1.,  0.,  1.,  1.,  1.,\n",
       "       -1.,  0., -1.,  1., -1.,  0.,  0., -1.,  0.,  0.,  1., -1.,  0.,\n",
       "        0., -1.,  0., -1.,  1.,  0.,  0., -1.,  0.,  1.,  1.,  0., -1.,\n",
       "        0.,  0.,  0.,  1.,  0., -1.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,\n",
       "        1.,  1.,  0., -1.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,\n",
       "        0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  1.,\n",
       "        0., -1., -1., -1.,  0.,  1.,  1., -1.,  0.,  1.,  1., -1., -1.,\n",
       "        0.,  0.,  1.,  0., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  1.,\n",
       "        0.,  0.,  0.,  1.,  0.,  0.,  0.,  0., -1., -1., -1.,  0.,  0.,\n",
       "        1., -1.,  1.,  0.,  1.,  0.,  0., -1., -1., -1.,  0., -1., -1.,\n",
       "        1., -1.,  1.,  0.,  0.,  0., -1., -1., -1.,  0., -1., -1.,  0.,\n",
       "       -1., -1.,  0.,  1.,  0.,  1., -1.,  1.,  0., -1., -1., -1., -1.,\n",
       "        0.,  1.,  1., -1.,  1.,  1., -1.,  0.,  0.,  0.,  0., -1.,  0.,\n",
       "       -1.,  1., -1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0., -1.,\n",
       "        0., -1.,  1., -1., -1.,  1.,  0.,  0.,  0.,  1., -1.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  1., -1.,  0.,  0.,  0.,\n",
       "        0.,  0.,  1.,  0.,  1.,  1.,  1., -1.,  0.,  1.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  1.,  1.,  0.,  1., -1., -1.,  1.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0., -1.,  1.,  1.,  0.,  0., -1., -1.,  0.,  0.,\n",
       "        0.,  1.,  0.,  0., -1.,  1., -1.,  1., -1.,  1.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  1., -1.,  0.,  0.,  1.,  0.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_init = 0.1\n",
    "meta_step_size = 1e-3\n",
    "trace_decay = 0.95\n",
    "gamma = 0.1 # 0.1\n",
    "eta = 0.5\n",
    "lr_decay = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StepData = namedtuple('StepData', ['input', 'reward', 'prediction'])\n",
    "history = []\n",
    "\n",
    "agent = Agent()\n",
    "\n",
    "prev_obs, env_state = init_env()\n",
    "prev_obs = agent.perception(prev_obs)\n",
    "\n",
    "trace, p, h, prev_h, trace, trace_bar = [torch.zeros_like(prev_obs) for _ in range(6)]\n",
    "beta = torch.empty_like(prev_obs).fill_(np.log(lr_init))\n",
    "lr = torch.exp(beta)\n",
    "old_value_pred = 0\n",
    "\n",
    "for _ in range(2000):\n",
    "    # Env step\n",
    "    obs, reward, env_state = env_step(env_state)\n",
    "    obs = agent.perception(obs)\n",
    "    obs[1] = 0\n",
    "\n",
    "    # Update weights\n",
    "    prev_value_pred = (agent.value_weights @ prev_obs).squeeze()\n",
    "    value_pred = (agent.value_weights @ obs).squeeze()\n",
    "    value_diff = prev_value_pred - old_value_pred\n",
    "    td_error = reward + gamma * value_pred - prev_value_pred\n",
    "    trace_value = torch.dot(trace, prev_obs)\n",
    "    lr_scale_max = torch.max(torch.tensor(eta), torch.sum(lr * prev_obs ** 2))\n",
    "\n",
    "    lr = torch.exp(beta)\n",
    "    trace *= gamma * trace_decay\n",
    "    trace += (eta / lr_scale_max) * lr * prev_obs \\\n",
    "        - lr * gamma * trace_decay * trace_value * prev_obs\n",
    "    agent.value_weights += td_error * trace - lr * prev_obs * value_diff\n",
    "\n",
    "    # Step-size optimization\n",
    "    p = trace_decay * gamma * p + prev_obs * h\n",
    "    beta += meta_step_size / (lr + 1e-8) * td_error * p\n",
    "    trace_bar *= gamma * trace_decay\n",
    "    trace_bar += lr * prev_obs * (\n",
    "        1 - gamma * trace_decay * trace_value \\\n",
    "            - gamma * trace_decay * prev_obs * trace_bar\n",
    "    )\n",
    "    k = h.clone()\n",
    "    h = h * (1 - lr * prev_obs ** 2) \\\n",
    "        - prev_h * prev_obs * (trace - prev_obs * lr) \\\n",
    "        + td_error * trace_bar - prev_obs * lr * value_diff\n",
    "    prev_h = k\n",
    "    \n",
    "    lr_decay_mask = ((lr_scale_max > eta) & (prev_obs != 0)).float()\n",
    "    beta -= torch.log(torch.tensor(lr_decay)) * lr_decay_mask\n",
    "\n",
    "    # Log\n",
    "    history.append(StepData(prev_obs, reward, prev_value_pred))\n",
    "\n",
    "    # Update for next iter\n",
    "    old_value_pred = value_pred\n",
    "    prev_obs = obs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
